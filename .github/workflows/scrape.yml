name: Run daily scraper

on:
  schedule:
    - cron: "10 7 * * *"   # 07:10 UTC daily
  workflow_dispatch:

jobs:
  run:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper (safe, no-guess)
        env:
          GOOGLE_SA_JSON_B64: ${{ secrets.GOOGLE_SA_JSON_B64 }}
          SHEET_ID:           ${{ secrets.SHEET_ID }}
          SHEET_TAB:          ${{ secrets.SHEET_TAB || 'Sheet1' }}
          DEFAULT_LOCATION:   ${{ secrets.DEFAULT_LOCATION || 'Ely' }}
          MAX_ROWS:           ${{ secrets.MAX_ROWS || '100' }}

          GOOGLE_CSE_KEY:     ${{ secrets.GOOGLE_CSE_KEY }}
          GOOGLE_CSE_CX:      ${{ secrets.GOOGLE_CSE_CX }}
          GOOGLE_CSE_QPS_DELAY_MS: ${{ secrets.GOOGLE_CSE_QPS_DELAY_MS || '800' }}
          GOOGLE_CSE_MAX_RETRIES:  ${{ secrets.GOOGLE_CSE_MAX_RETRIES  || '5' }}

          # Crawl behavior
          HTTP_TIMEOUT: "15"
          MAX_PAGES_PER_SITE: "15"
          MIN_PAGES_BEFORE_FALLBACK: "5"

          # Absolutely no guessing
          ALLOW_GUESS: "false"
          PREFER_COMPANY_DOMAIN: "true"
        run: python -m src.main
