name: Run daily scraper

on:
  schedule:
    - cron: "10 7 * * *"   # 07:10 UTC daily
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper (FAST MODE)
        env:
          FAST_MODE:             "1"   # <â€” make it brisk
          GOOGLE_SA_JSON_B64:    ${{ secrets.GOOGLE_SA_JSON_B64 }}
          SHEET_ID:              ${{ secrets.SHEET_ID }}
          SHEET_TAB:             ${{ secrets.SHEET_TAB || 'Sheet1' }}
          DEFAULT_LOCATION:      ${{ secrets.DEFAULT_LOCATION || 'Ely' }}
          MAX_ROWS:              ${{ secrets.MAX_ROWS || '100' }}
          GOOGLE_CSE_KEY:        ${{ secrets.GOOGLE_CSE_KEY }}
          GOOGLE_CSE_CX:         ${{ secrets.GOOGLE_CSE_CX }}
          GOOGLE_CSE_QPS_DELAY_MS: ${{ secrets.GOOGLE_CSE_QPS_DELAY_MS || '600' }}
          GOOGLE_CSE_MAX_RETRIES:  ${{ secrets.GOOGLE_CSE_MAX_RETRIES || '2' }}
          BING_API_KEY:          ${{ secrets.BING_API_KEY }}
          SCRAPERAPI_KEY:        ${{ secrets.SCRAPERAPI_KEY }}
        run: python -m src.main
