name: Scrape Contacts

on:
  schedule:
    - cron: '10 7 * * *'   # 07:10 UTC every day
  workflow_dispatch:

jobs:
  scrape:
    name: Run daily scraper
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read
    concurrency:
      group: scrape-contacts
      cancel-in-progress: false

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Run scraper
        env:
          PYTHONUNBUFFERED: '1'

          # Required
          GOOGLE_SA_JSON_B64:             ${{ secrets.GOOGLE_SA_JSON_B64 }}
          SHEET_ID:                       ${{ secrets.SHEET_ID }}

          # Safe defaults so blanks don't crash
          SHEET_TAB:                      ${{ secrets.SHEET_TAB || 'Sheet1' }}
          DEFAULT_LOCATION:               ${{ secrets.DEFAULT_LOCATION || 'Ely' }}
          MAX_ROWS:                       ${{ secrets.MAX_ROWS || '100' }}

          # Google Programmable Search (CSE)
          GOOGLE_CSE_KEY:                 ${{ secrets.GOOGLE_CSE_KEY }}
          GOOGLE_CSE_CX:                  ${{ secrets.GOOGLE_CSE_CX }}

          # Backoff to reduce transient 429s
          GOOGLE_CSE_QPS_DELAY_MS:        ${{ secrets.GOOGLE_CSE_QPS_DELAY_MS || '800' }}
          GOOGLE_CSE_MAX_RETRIES:         ${{ secrets.GOOGLE_CSE_MAX_RETRIES || '5' }}

          # Optional fallbacks (leave empty if unused)
          BING_API_KEY:                   ${{ secrets.BING_API_KEY || '' }}
          SCRAPERAPI_KEY:                 ${{ secrets.SCRAPERAPI_KEY || '' }}
        run: python -m src.main
